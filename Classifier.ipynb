{
 "cells": [
 
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using findspark to get jupyter notebook to recognize spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the articles csv\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('./data/shuffled_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        article_text|category|\n",
      "+--------------------+--------+\n",
      "|PhotoGood morning...|politics|\n",
      "|A version of this...|  sports|\n",
      "|GLENN KENNY The o...|  movies|\n",
      "|Federal accident ...|business|\n",
      "|Ken Hechler, a le...|politics|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_text: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the categories\n",
    "from pyspark.sql.functions import col\n",
    "#data.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize and get vectors\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"article_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# getting stop words from nltk corpus\n",
    "from nltk.corpus import stopwords\n",
    "add_stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stop_words)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|        article_text|category|               words|            filtered|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|PhotoGood morning...|politics|[photogood, morni...|[photogood, morni...|(4152,[0,1,2,3,4,...|  1.0|\n",
      "|A version of this...|  sports|[a, version, of, ...|[version, article...|(4152,[1,8,22,37,...|  2.0|\n",
      "|GLENN KENNY The o...|  movies|[glenn, kenny, th...|[glenn, kenny, op...|(4152,[0,3,4,11,1...|  3.0|\n",
      "|Federal accident ...|business|[federal, acciden...|[federal, acciden...|(4152,[2,10,17,61...|  0.0|\n",
      "|Ken Hechler, a le...|politics|[ken, hechler, a,...|[ken, hechler, le...|(4152,[0,2,5,8,11...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# setting up the pipeline\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 395\n",
      "Test Dataset Count: 80\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility and doing a 80-20 training/test split\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 279)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-DIF\n",
    "# Extracting features\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                  article_text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|For devotees of President T...|business|[1.0,4.453107315457469E-19,...|  0.0|       0.0|\n",
      "|MEAN bosses could have kill...|business|[1.0,2.5394656950292945E-21...|  0.0|       0.0|\n",
      "|PARIS � The eurozone econom...|business|[1.0,1.5951862611678954E-22...|  0.0|       0.0|\n",
      "|BUT for starters, as anyone...|business|[1.0,1.5218498006537328E-24...|  0.0|       0.0|\n",
      "|To the Editor:Mr. Obama has...|business|[1.0,5.653790142303447E-30,...|  0.0|       0.0|\n",
      "|You have to respect America...|politics|[1.0,2.6293152673047053E-33...|  1.0|       0.0|\n",
      "|President Trump faces a cho...|business|[1.0,3.576130617812927E-34,...|  0.0|       0.0|\n",
      "|LONDON � Barclays announced...|business|[1.0,3.445015165588454E-35,...|  0.0|       0.0|\n",
      "|WASHINGTON � Small-business...|business|[1.0,9.516155657707138E-42,...|  0.0|       0.0|\n",
      "|If Greece�s European credit...|politics|[1.0,1.9422030574214946E-44...|  1.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fitting a naive bayes classifier \n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"article_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7610570824524312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the naive bayes classifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                  article_text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|LONDON � The French insurer...|business|[0.4317033239799247,0.17201...|  0.0|       0.0|\n",
      "|WASHINGTON � Small-business...|business|[0.4114852273958765,0.17811...|  0.0|       0.0|\n",
      "|The inaction helped Toshiba...|business|[0.4114624849360925,0.18102...|  0.0|       0.0|\n",
      "|For devotees of President T...|business|[0.40304342756103007,0.2053...|  0.0|       0.0|\n",
      "|Airbus said Monday it was i...|business|[0.40285660071443663,0.2047...|  0.0|       0.0|\n",
      "|The Business Insider boss H...|business|[0.39842789023948344,0.1882...|  0.0|       0.0|\n",
      "|Since President Obama moved...|business|[0.37630731023653247,0.2327...|  0.0|       0.0|\n",
      "|President Trump faces a cho...|business|[0.37509000964707495,0.2189...|  0.0|       0.0|\n",
      "|The website has seed fundin...|business|[0.36967110988199325,0.2113...|  0.0|       0.0|\n",
      "|LONDON � Barclays announced...|business|[0.3679461156065507,0.19949...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"article_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6661218487394958"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the random forest classifier\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
